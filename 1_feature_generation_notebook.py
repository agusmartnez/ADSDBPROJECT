# -*- coding: utf-8 -*-
"""1. feature_generation_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_JzzhTkdXsbXX9S7TYx5ZksV9gwOuflU

#  FEATURE GENERATION ZONE

##### Steps to access to the DB store in my local host:
"""

#!pip install psycopg2
#!pip install pandas

"""https://research.google.com/colaboratory/intl/en-GB/local-runtimes.html

<b>1. Create a virtual env (en cmd)</b>
- C:\Users\Andrea> pip install virtualenv
- C:\Users\Andrea> python3 -m venv env
- C:\Users\Andrea> cd C:\Users\Andrea\env\Scripts
- C:\Users\Andrea> activate.bat

<b>2. Connect to local runtime</b>
- (env) C:\Users\Andrea\env> pip install jupyter
- (env) C:\Users\Andrea\env> pip install jupyter_http_over_ws
- (env) C:\Users\Andrea\env> jupyter serverextension enable --py jupyter_http_over_ws
- (env) C:\Users\Andrea\env> jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0

<b>3. In Google Colab (do not close cmd)</b>
- In the right, next to 'Editar', open the tab <u>Conectar a un entorno de ejecuci√≥n local</u>
- Copy from the cmd <i>http://localhost:8888/?token=8dhfyd</i> and connect.

## Installs and imports
"""

!pip install psycopg2
!pip install pandas

import psycopg2
from psycopg2 import Error
import pandas as pd

"""## Get the data from the database in PostgreSQL"""

try:
    connection = psycopg2.connect(user="adsdb",
                                  password="adsdb",
                                  host="localhost",
                                  port="5432",
                                  database="adsdb_analysis")

    cursor = connection.cursor()     
    cursor.execute("select * from df_atur_barcelona_exploitation_csv")
    record = cursor.fetchall()
    df_atur_barcelona_exploitation_csv = pd.DataFrame(record, columns=['column1','Year', 'quarter', 'code_district', 'name_district', 'code_neighborhood', 'name_neighborhood', 'gender', 'demmand_occupancy', 'total'])

    cursor.execute("select * from df_atur_education_exploitation_csv")
    record = cursor.fetchall()
    df_atur_education_exploitation_csv = pd.DataFrame(record, columns=['column1', 'Year', 'ccaa', 'gender','education_sector', 'rate', 'Total(per Year, CCA, Gender)'])

    cursor.execute("select * from df_atur_zone_exploitation_csv")
    record = cursor.fetchall()
    df_atur_zone_exploitation_csv = pd.DataFrame(record, columns=['column1', 'Year', 'quarter', 'Zone', 'gender', 'total'])
    

except (Exception, Error) as error:
    print("Error while connecting to PostgreSQL", error)
finally:
    if (connection):
        cursor.close()
        connection.close()
        print("PostgreSQL connection is closed")

"""### Head of the data sets"""

# removing features not needed for the analysis
df_atur_barcelona_exploitation_csv = df_atur_barcelona_exploitation_csv.drop(columns=['column1', 'code_district', 'code_neighborhood', 'quarter', 'name_district', 'gender'])
df_atur_barcelona_exploitation_csv.head()

#remove column1
df_atur_education_exploitation_csv = df_atur_education_exploitation_csv.drop(columns=['column1'])
df_atur_education_exploitation_csv.head()

#remove column1
df_atur_zone_exploitation_csv = df_atur_zone_exploitation_csv.drop(columns=['column1', 'gender'])
df_atur_zone_exploitation_csv.head()

"""## Save the results

We save the results in our local computer due to we can not connect this notebook to Google Drive. This datasets will be uploaded to the landing folder, into the `1. output formatted zone` folder.
"""

df_atur_barcelona_exploitation_csv.to_csv('/Users/meritxellarbiollopez/Desktop/df_atur_barcelona_featgen.csv')
df_atur_education_exploitation_csv.to_csv('/Users/meritxellarbiollopez/Desktop/df_atur_education_featgen.csv')
df_atur_zone_exploitation_csv.to_csv('/Users/meritxellarbiollopez/Desktop/df_atur_zone_featgen.csv')